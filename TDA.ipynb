{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP-QGjB3e9DX"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# **Translational Data Augmentation** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Uplaod the torch folder in MyDrive**"
      ],
      "metadata": {
        "id": "MXyV7I9txNTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN-xDfWS3S2t",
        "outputId": "d58ceda5-b8b5-4bea-8c2f-a0812db95047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Clone or Upload DataAugmentationNMT folder in MyDrive**"
      ],
      "metadata": {
        "id": "L90hHtejx_-e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvuPu8N48Jao"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/marziehf/DataAugmentationNMT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pocPNOMm-gI"
      },
      "source": [
        "# **Step 1: Data Preprocessing**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**The input files train.en.cln , test.en.cln , dev.en.cln , vocab_freq.trg are uploaded in DataAugmentation folder**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before training the monolingual language model in [src/trg] you'll need to preprocess the data for both forward and backward direction using preprocess.no_preset_v.py.**"
      ],
      "metadata": {
        "id": "7bovFWhTzuWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fRhEsp6_4xv",
        "outputId": "c57c6f7e-64b5-42ce-8e9c-e84aa7f6c941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DataAugmentationNMT\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/DataAugmentationNMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61IXFA8wa6Dc"
      },
      "outputs": [],
      "source": [
        "!python src/preprocess.no_preset_v.py --train_txt /content/sample_data/train.en \\\n",
        "--val_txt /content/sample_data/dev.en --test_txt /content/sample_data/test.en \\\n",
        "--output_h5 ./data.h5 --output_json ./data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOjkKn9Sx8mr"
      },
      "source": [
        "**Reverse the data for backward training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgWmG-aY0Glk"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/DataAugmentationNMT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq78i0bLdnmT"
      },
      "outputs": [],
      "source": [
        "!cat /content/gdrive/MyDrive/DataAugmentationNMT/train.en.te.cln | awk '{for (i=NF; i>0; i--) {printf \"%s \", $i;} printf \"\\n\" }' > train.en.rev.cln\n",
        "!cat /content/gdrive/MyDrive/DataAugmentationNMT/test.en.te.cln | awk '{for (i=NF; i>0; i--) {printf \"%s \", $i;} printf \"\\n\" }' > test.en.rev.cln\n",
        "!cat /content/gdrive/MyDrive/DataAugmentationNMT/dev.en.te.cln| awk '{for (i=NF; i>0; i--) {printf \"%s \", $i;} printf \"\\n\" }' > dev.en.rev.cln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKnTL3yaK0Ep"
      },
      "outputs": [],
      "source": [
        "!python /content/gdrive/MyDrive/DataAugmentationNMT/src/preprocess.no_preset_v.py --train_txt ./train.en.rev.cln \\\n",
        "--val_txt ./dev.en.rev.cln --test_txt ./test.en.rev.cln \\\n",
        "--output_h5 ./data.rev.h5 --output_json ./data.rev.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4pxYKJpekoQ"
      },
      "source": [
        "# **Step 2: Language Model Training in forward and backward direction** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iJKFDlw-4mz"
      },
      "source": [
        "**Generation of vocabulary frequency list**\n",
        "\n",
        "The vocabfreq input is the frequency list of words in the low-resource setting that need augmentation later on using these language models. The format is:\n",
        "\n",
        "...\n",
        "\n",
        "**change 3028**\n",
        "\n",
        "**taken 3007**\n",
        "\n",
        "**large 2999**\n",
        "\n",
        "**again 2994**\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Script for Vocab Frequency file***"
      ],
      "metadata": {
        "id": "NuPiwDz41VIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEwRSPzU_CHz",
        "outputId": "6b153201-d0e4-43e8-e781-ccd6ef98cb57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string \n",
        " \n",
        "# Open the file in read mode \n",
        "text = open(\"/content/gdrive/MyDrive/DataAugmentationNMT/ta/train.en.cln\", \"r\") \n",
        " \n",
        "# Create an empty dictionary \n",
        "d = dict() \n",
        " \n",
        "# Loop through each line of the file \n",
        "for line in text: \n",
        "    # Remove the leading spaces and newline character \n",
        "    line = line.strip() \n",
        " \n",
        "    # Convert the characters in line to \n",
        "    # lowercase to avoid case mismatch \n",
        "    line = line.lower() \n",
        " \n",
        "    # Remove the punctuation marks from the line  \n",
        " \n",
        "    # Split the line into words \n",
        "    words = line.split(\" \") \n",
        " \n",
        "    # Iterate over each word in line \n",
        "    for word in words: \n",
        "        # Check if the word is already in dictionary \n",
        "        if word in d: \n",
        "            # Increment count of word by 1 \n",
        "            d[word] = d[word] + 1\n",
        "        else: \n",
        "            # Add the word to dictionary with count 1 \n",
        "            d[word] = 1\n",
        "import sys\n",
        "from __future__ import print_function\n",
        "f = open(\"vocab_freq.trg\" , 'w')\n",
        "sys.stdout =f\n",
        "# Print the contents of dictionary \n",
        "for key in sorted(d, key=d.get, reverse=True): \n",
        "  sys.stdout =f\n",
        "  print(key,\" \",d[key],sep=\"\") \n",
        " \n",
        "f.close"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After preprocessing the data you'll need to train two language models in forward and backward directions.**"
      ],
      "metadata": {
        "id": "PpxHHSb52ICa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlxdUv5xYp7U"
      },
      "outputs": [],
      "source": [
        "!/content/gdrive/MyDrive/torch/install/bin/th /content/gdrive/MyDrive/torch/DataAugmentationNMT/src/train.lua -input_h5 data.h5 -input_json data.json \\\n",
        "-checkpoint_name /content/gdrive/MyDrive/DataAugmentationNMT/models_rnn/cv  -vocabfreq vocab_freq.trg -max_epochs 50 -print_every 2000 -gpu -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/gdrive/MyDrive/torch/install/bin/th /content/gdrive/MyDrive/torch/DataAugmentationNMT/src/train.lua -input_h5 data.rev.h5 -input_json data.rev.json \\\n",
        "-checkpoint_name /content/gdrive/MyDrive/DataAugmentationNMT/models_rnn_rev/cv  -vocabfreq vocab_freq.trg -max_epochs 50 -print_every 2000 -gpu -1"
      ],
      "metadata": {
        "id": "nUuoUkk9dSHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of0omCVMfHuq"
      },
      "source": [
        "# **Step 3: Substitution Generation**\n",
        "\n",
        "**start_text** is the side of the bitext that you are targeting for augmentation of rare words. \n",
        "\n",
        "**vocabfreq** is the frequency list used for detecting rare words. \n",
        "\n",
        "**topk** indicates the maximum number of substitutions you want to have for each position in the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After training the language models you can generate new sentences in your bitext for [src\\trg]. You can run this:**"
      ],
      "metadata": {
        "id": "5sHii-IU2nDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqeDIqrJKSQ4"
      },
      "outputs": [],
      "source": [
        "!/content/gdrive/MyDrive/torch/install/bin/th ./src/substitution.lua -checkpoint /content/gdrive/MyDrive/DataAugmentationNMT/models_rnn/cv_100000.t7 -start_text /train.en.cln -vocabfreq /vocab_freq.ta.trg -sample 0 -topk 25 -bwd 0  > /subs.en -gpu -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/gdrive/MyDrive/torch/install/bin/th ./src/substitution.lua -checkpoint /content/gdrive/MyDrive/DataAugmentationNMT/models_rnn_rev/cv_100000.t7 -start_text /train.en.rev.cln -vocabfreq /vocab_freq.trg -sample 0 -topk 25 -bwd 0  > /subs.en.rev -gpu -1"
      ],
      "metadata": {
        "id": "EKXlVYd6BWAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxc5wX9wgT72"
      },
      "source": [
        "**Running these two codes will give you augmented corpora with a list of substitutions on one side subs.en and subs.en.rev In order to find substitions that best match the context, you'll need to find the intersection of these two lists:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWoc8eH2gx1z"
      },
      "outputs": [],
      "source": [
        "!perl ./scripts/generate_intersect.pl /subs.en /subs.en.rev /subs.en.intersect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwKIQYz_hYH5"
      },
      "source": [
        "# **Step 4: Generate Augmented corpora**\n",
        "\n",
        "Using the substitution output, the [trg/src] side of the bitext, the alignment, and the lexical probability file you can generate the augmented corpora.\n",
        "\n",
        "You can use fast_align to obtain alignments for your bitext. The format of the alignment input is:\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "0-0 1-10 2-3 2-4 2-5 3-13 4-14 5-8 5-9 6-16 7-14 8-11 10-6 11-7 12-17\n",
        "\n",
        "0-0 1-0 2-0 2-2 3-1 3-3 4-5 5-5 6-6 8-8 9-9 10-10 11-11\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcnxbBRv_zUw"
      },
      "source": [
        "**fast_align** to obtain alignments for your bitext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7YwWGD2AGy4"
      },
      "outputs": [],
      "source": [
        "%cd /content/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU2ZNZo9_x8P"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/clab/fast_align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ki2OV2EAYL-"
      },
      "outputs": [],
      "source": [
        "%cd /content/fast_align\n",
        "%mkdir build\n",
        "%cd /content/fast_align/build\n",
        "!cmake ..\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6xWGySwApRD"
      },
      "source": [
        " **Input format for fast align**\n",
        "\n",
        "Input to fast_align must be tokenized and aligned into parallel sentences. Each line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space **(|||)**. An example 3-sentence Germanâ€“English parallel corpus is:\n",
        "\n",
        "**doch jetzt ist der Held gefallen . ||| but now the hero has fallen .**\n",
        "\n",
        "**neue Modelle werden erprobt . ||| new models are being tested .**\n",
        "\n",
        "**doch fehlen uns neue Ressourcen . ||| but we lack new resources**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZlwYMA4A4NM"
      },
      "outputs": [],
      "source": [
        "!./fast_align -i /content/fast_align/build/bilingual.txt -d -o -v  > /content/DataAugmentationNMT/alignment.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQsqyrsPBope"
      },
      "outputs": [],
      "source": [
        "!./fast_align -i /content/fast_align/build/bilingual.txt -d -o -v -p /content/DataAugmentationNMT/lex.prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmTVdqmXB10t"
      },
      "outputs": [],
      "source": [
        "%cd /content/DataAugmentationNMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WqFhtxxB7Ta"
      },
      "source": [
        "**To covert logrithimic lexical probability table into lexical probability table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O0zz84jCCsP"
      },
      "outputs": [],
      "source": [
        "import string \n",
        "import math\n",
        "import sys\n",
        "from __future__ import print_function\n",
        " \n",
        "# Open the file in read mode \n",
        "text = open(\"lex.prob\", \"r\") \n",
        "\n",
        "# Loop through each line of the file \n",
        "for line in text: \n",
        "    # Remove the leading spaces and newline character \n",
        "    line = line.strip()\n",
        "    # Split the line into words \n",
        "    words = line.split()\n",
        "    if len(words) >= 3:\n",
        "      prob = words[2] \n",
        "      p= math.exp(float(prob))\n",
        "      p=round(p,5)\n",
        "      words= words[0] + \" \" + words[1] + \" \" + str(p)\n",
        "      line = line.replace(line , words)\n",
        "      with open('lex.txt', 'a') as f:\n",
        "        print(line, file=f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Augmented Corpa**"
      ],
      "metadata": {
        "id": "l0wCmzUf5382"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K42SwOJ3Vyof"
      },
      "outputs": [],
      "source": [
        "!perl ./scripts/data_augmentation.pl /content/gdrive/MyDrive/DataAugmentationNMT/ta/subs.en.intersect /train.en.cln /alignment.txt /lex.txt /content/gdrive/MyDrive/DataAugmentationNMT/augmentedOutput.en"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This will generate two files: augmentedOutput.augmented in [src/trg] and augmentedOutput.fillout in [trg/src] language. The first file is the side of the bitext augmented targeting the rare words. The second file is respective translations of the augmented sentences.**"
      ],
      "metadata": {
        "id": "JP-ELLeq6by5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcvVEQXxiQMF"
      },
      "source": [
        "**If you want to have more than one change in each sentence you can also run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fQeTyxjiV5E"
      },
      "outputs": [],
      "source": [
        "!perl ./scripts/data_augmentation_multiplechanges.pl subs.intersect train.hi.cln alignment.txt lex.txt augmentedOutput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWDsKYzMinbn"
      },
      "source": [
        "# **Step 5:Generate Clean Bitext for Translation**\n",
        "\n",
        "Generate Clean Bitext for Translation\n",
        "To remove all markups and have clean bitext that can be used for translation training you can run: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yf0_HRH8laq"
      },
      "outputs": [],
      "source": [
        "!perl ./scripts/filter_out_augmentations.pl augmentedOutput.en.augment.multmax.aug augmentedOutput.en.fillout.multmax.aug 1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7A4-c2JxrJz"
      },
      "source": [
        "**You can impose further frequncy limit on rare words you want to augment here. The default frequency of rare word is 1000**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}